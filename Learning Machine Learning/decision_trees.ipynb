{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a44925f8-6501-452a-ad4d-fb824b4266d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, mode='classification'):\n",
    "        \"\"\"\n",
    "        Initialize Decision Tree.\n",
    "        \n",
    "        Parameters:\n",
    "        - max_depth: Maximum depth of the tree (default: None, no limit)\n",
    "        - min_samples_split: Minimum number of samples required to split a node (default: 2)\n",
    "        - mode: 'classification' or 'regression' (default: 'classification')\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.mode = mode\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Decision Tree.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Feature matrix (n_samples, n_features)\n",
    "        - y: Target values (n_samples,)\n",
    "        \"\"\"\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained Decision Tree.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Feature matrix (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        - Predictions (n_samples,)\n",
    "        \"\"\"\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        \"\"\"\n",
    "        Recursively build the Decision Tree.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        n_labels = len(np.unique(y))\n",
    "\n",
    "        # Stopping conditions\n",
    "        if (self.max_depth is not None and depth >= self.max_depth) or \\\n",
    "           n_samples < self.min_samples_split or \\\n",
    "           n_labels == 1:\n",
    "            return self._create_leaf(y)\n",
    "\n",
    "        # Find the best split\n",
    "        best_split = self._find_best_split(X, y, n_samples, n_features)\n",
    "        if not best_split:\n",
    "            return self._create_leaf(y)\n",
    "\n",
    "        # Split the data\n",
    "        left_idxs, right_idxs = best_split['left_idxs'], best_split['right_idxs']\n",
    "        left = self._build_tree(X[left_idxs, :], y[left_idxs], depth + 1)\n",
    "        right = self._build_tree(X[right_idxs, :], y[right_idxs], depth + 1)\n",
    "\n",
    "        return {\n",
    "            'feature_index': best_split['feature_index'],\n",
    "            'threshold': best_split['threshold'],\n",
    "            'left': left,\n",
    "            'right': right\n",
    "        }\n",
    "\n",
    "    def _find_best_split(self, X, y, n_samples, n_features):\n",
    "        \"\"\"\n",
    "        Find the best split for the data.\n",
    "        \"\"\"\n",
    "        best_split = {}\n",
    "        best_gain = -float('inf')\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                left_idxs = np.where(X[:, feature_index] <= threshold)[0]\n",
    "                right_idxs = np.where(X[:, feature_index] > threshold)[0]\n",
    "\n",
    "                if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Calculate information gain or variance reduction\n",
    "                if self.mode == 'classification':\n",
    "                    gain = self._information_gain(y, y[left_idxs], y[right_idxs])\n",
    "                elif self.mode == 'regression':\n",
    "                    gain = self._variance_reduction(y, y[left_idxs], y[right_idxs])\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_split = {\n",
    "                        'feature_index': feature_index,\n",
    "                        'threshold': threshold,\n",
    "                        'left_idxs': left_idxs,\n",
    "                        'right_idxs': right_idxs,\n",
    "                        'gain': gain\n",
    "                    }\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    def _information_gain(self, y, y_left, y_right):\n",
    "        \"\"\"\n",
    "        Calculate information gain for classification.\n",
    "        \"\"\"\n",
    "        p = len(y_left) / len(y)\n",
    "        return self._entropy(y) - p * self._entropy(y_left) - (1 - p) * self._entropy(y_right)\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        \"\"\"\n",
    "        Calculate entropy for classification.\n",
    "        \"\"\"\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "    def _variance_reduction(self, y, y_left, y_right):\n",
    "        \"\"\"\n",
    "        Calculate variance reduction for regression.\n",
    "        \"\"\"\n",
    "        p = len(y_left) / len(y)\n",
    "        return np.var(y) - p * np.var(y_left) - (1 - p) * np.var(y_right)\n",
    "\n",
    "    def _create_leaf(self, y):\n",
    "        \"\"\"\n",
    "        Create a leaf node.\n",
    "        \"\"\"\n",
    "        if self.mode == 'classification':\n",
    "            return Counter(y).most_common(1)[0][0]  # Majority class\n",
    "        elif self.mode == 'regression':\n",
    "            return np.mean(y)  # Mean value\n",
    "\n",
    "    def _traverse_tree(self, x, node):\n",
    "        \"\"\"\n",
    "        Traverse the tree to make a prediction.\n",
    "        \"\"\"\n",
    "        if isinstance(node, dict):\n",
    "            feature_index = node['feature_index']\n",
    "            threshold = node['threshold']\n",
    "            if x[feature_index] <= threshold:\n",
    "                return self._traverse_tree(x, node['left'])\n",
    "            else:\n",
    "                return self._traverse_tree(x, node['right'])\n",
    "        else:\n",
    "            return node  # Leaf node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fad1d180-1bd2-4bba-bf21-ab0980c40e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classification Example\n",
      "===================================\n",
      "Predictions: [0 1 0 1 1 1]\n",
      "Accuracy: 1.00\n",
      "\n",
      "Decision Tree Regression Example\n",
      "================================\n",
      "Predictions: [ 2.  4.  6.  8. 10. 12.]\n",
      "MSE: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Classification example\n",
    "    print(\"Decision Tree Classification Example\")\n",
    "    print(\"===================================\")\n",
    "    X = np.array([[2, 3], [10, 15], [3, 4], [6, 8], [7, 10], [8, 12]])\n",
    "    y = np.array([0, 1, 0, 1, 1, 1])\n",
    "\n",
    "    clf = DecisionTree(max_depth=3, mode='classification')\n",
    "    clf.fit(X, y)\n",
    "    predictions = clf.predict(X)\n",
    "    print(f\"Predictions: {predictions}\")\n",
    "    print(f\"Accuracy: {np.mean(predictions == y):.2f}\")\n",
    "\n",
    "    # Regression example\n",
    "    print(\"\\nDecision Tree Regression Example\")\n",
    "    print(\"================================\")\n",
    "    X = np.array([[1], [2], [3], [4], [5], [6]])\n",
    "    y = np.array([2, 4, 6, 8, 10, 12])\n",
    "\n",
    "    reg = DecisionTree(max_depth=3, mode='regression')\n",
    "    reg.fit(X, y)\n",
    "    predictions = reg.predict(X)\n",
    "    print(f\"Predictions: {predictions}\")\n",
    "    print(f\"MSE: {np.mean((predictions - y) ** 2):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca779a-2cb3-4ea4-98d8-0a1f27355c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
